{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see some code - Interacting with the environment\n",
    "\n",
    "The code snippet below shows how to initialise the environment and step through in an 'offline' manner\n",
    "(Here offline means that the environment is generating some recommendations for us).\n",
    "We print out the results from the environment at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 0 - ACTION: None - OBSERVATION: [{'t': 0, 'u': 0, 'z': 'pageview', 'v': 0}] - REWARD: None\n",
      "STEP: 1 - ACTION: {'t': 1, 'u': 0, 'a': 3, 'ps': 0.1, 'ps-a': ()} - OBSERVATION: [] - REWARD: 0\n",
      "STEP: 2 - ACTION: {'t': 2, 'u': 0, 'a': 4, 'ps': 0.1, 'ps-a': ()} - OBSERVATION: [] - REWARD: 0\n",
      "STEP: 3 - ACTION: {'t': 3, 'u': 0, 'a': 5, 'ps': 0.1, 'ps-a': ()} - OBSERVATION: [] - REWARD: 0\n"
     ]
    }
   ],
   "source": [
    "import gym, recogym\n",
    "\n",
    "# env_0_args is a dictionary of default parameters (i.e. number of products)\n",
    "from recogym import env_1_args, Configuration\n",
    "\n",
    "# You can overwrite environment arguments here:\n",
    "env_1_args['random_seed'] = 42\n",
    "\n",
    "# Initialize the gym for the first time by calling .make() and .init_gym()\n",
    "env = gym.make('reco-gym-v1')\n",
    "env.init_gym(env_1_args)\n",
    "\n",
    "# .reset() env before each episode (one episode per user).\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "# Counting how many steps.\n",
    "i = 0\n",
    "\n",
    "observation, reward, done = None, 0, False\n",
    "while not done:\n",
    "    action, observation, reward, done, info = env.step_offline(observation, reward, done)\n",
    "    print(f\"STEP: {i} - ACTION: {action} - OBSERVATION: {observation.sessions()} - REWARD: {reward}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, there's quite a bit going on here:\n",
    "- `action`, is a number between `0` and `num_products - 1` that references the index of the product recommended.\n",
    "- `observation` will either be `None` or a session of Organic data, showing the index of products the user views.\n",
    "- `reward` is 0 if the user does not click on the recommended product and 1 if they do. Notice that when a user clicks on a product (Wherever the reward is 1), they start a new Organic session.\n",
    "- `done` is a True/False flag indicating if the episode (aka user's timeline) is over.\n",
    "- `info` currently not used, so it is always an empty dictionary.\n",
    "\n",
    "Also, notice that the first `action` is `None`.  In our implementation, the agent observes Organic behaviour before\n",
    "recommending anything.\n",
    "\n",
    "Now, we will show calling the environment in an online manner, where the agent needs to supply an action.\n",
    "For demonstration purposes, we will create a list of hard-coded actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 - Action: None - Observation: [{'t': 0, 'u': 0, 'z': 'pageview', 'v': 1}] - Reward: None\n",
      "Step: 1 - Action: 1 - Observation: [] - Reward: 0\n",
      "Step: 2 - Action: 2 - Observation: [] - Reward: 0\n",
      "Step: 3 - Action: 3 - Observation: [] - Reward: 0\n",
      "Step: 4 - Action: 4 - Observation: [] - Reward: 0\n",
      "Step: 5 - Action: 5 - Observation: [{'t': 6, 'u': 0, 'z': 'pageview', 'v': 4}, {'t': 7, 'u': 0, 'z': 'pageview', 'v': 4}, {'t': 8, 'u': 0, 'z': 'pageview', 'v': 4}, {'t': 9, 'u': 0, 'z': 'pageview', 'v': 4}, {'t': 10, 'u': 0, 'z': 'pageview', 'v': 4}, {'t': 11, 'u': 0, 'z': 'pageview', 'v': 6}, {'t': 12, 'u': 0, 'z': 'pageview', 'v': 4}, {'t': 13, 'u': 0, 'z': 'pageview', 'v': 4}, {'t': 14, 'u': 0, 'z': 'pageview', 'v': 4}, {'t': 15, 'u': 0, 'z': 'pageview', 'v': 6}, {'t': 16, 'u': 0, 'z': 'pageview', 'v': 4}, {'t': 17, 'u': 0, 'z': 'pageview', 'v': 1}, {'t': 18, 'u': 0, 'z': 'pageview', 'v': 4}, {'t': 19, 'u': 0, 'z': 'pageview', 'v': 4}, {'t': 20, 'u': 0, 'z': 'pageview', 'v': 4}, {'t': 21, 'u': 0, 'z': 'pageview', 'v': 4}, {'t': 22, 'u': 0, 'z': 'pageview', 'v': 1}, {'t': 23, 'u': 0, 'z': 'pageview', 'v': 6}] - Reward: 0\n",
      "Step: 6 - Action: 6 - Observation: [] - Reward: 0\n",
      "Step: 7 - Action: 7 - Observation: [] - Reward: 0\n",
      "Step: 8 - Action: 8 - Observation: [] - Reward: 0\n"
     ]
    }
   ],
   "source": [
    "# Create list of hard coded actions.\n",
    "actions = [None] + [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# Reset env and set done to False.\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "# Counting how many steps.\n",
    "i = 0\n",
    "\n",
    "while not done and i < len(actions):\n",
    "    action = actions[i]\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(f\"Step: {i} - Action: {action} - Observation: {observation.sessions()} - Reward: {reward}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our Hello World Agent\n",
    "\n",
    "Now that we see have seen how the offline and online versions of the environment work,\n",
    "it is time to code our first recommendation agent!\n",
    "\n",
    "Technically, an agent can be anything that produces actions for the environment to use.\n",
    "However, we will show you the object-oriented way we like to create agents.\n",
    "\n",
    "Below is the code for a very simple agent\n",
    "The agent records merely how many times a user sees each product organically,\n",
    "then when required to make a recommendation, the agent chooses a product randomly in proportion with\n",
    "a number of times the user has viewed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "from recogym.agents import Agent\n",
    "\n",
    "# Define an Agent class.\n",
    "class HelloWorldAgent(Agent):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super(HelloWorldAgent, self).__init__(config) # Set number of products as an attribute of the Agent.\n",
    "\n",
    "        self.organic_views = np.zeros(self.config.num_products) # Track number of times each item viewed in Organic session.\n",
    "\n",
    "    def train(self, observation, action, reward, done):\n",
    "\n",
    "        # Train method learns from a tuple of data. This method can be called for offline or online learning\n",
    "        # Adding organic session to organic view counts.\n",
    "\n",
    "        if observation:\n",
    "            for session in observation.sessions():\n",
    "                self.organic_views[session['v']] += 1\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "\n",
    "        # Act method returns an action - based on current observation and past history\n",
    "        # Choosing action randomly in proportion with number of views.\n",
    "\n",
    "        prob = self.organic_views / sum(self.organic_views)\n",
    "\n",
    "        action = choice(self.config.num_products, p = prob) # GENERATE ONLINE ACTION\n",
    "\n",
    "        return {\n",
    "            **super().act(observation, reward, done),\n",
    "            **{\n",
    "                'a': action,\n",
    "                'ps': prob[action]\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `HelloWorldAgent` class above demonstrates our preferred way to create agents for RecoGym.\n",
    "\n",
    "Notice how we have both a `train` and `act` method present. \n",
    "\n",
    "The `train` method is designed to take in training data from the environments `step_offline` \n",
    "method and thus has nothing to return, while the `act` method must return an action to pass \n",
    "back into the environment.\n",
    "\n",
    "The code below highlights how one would use this agent for first offline training and then using the learned\n",
    "knowledge to make recommendations online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click Through Rate: 0.0132\n"
     ]
    }
   ],
   "source": [
    "# Instantiate instance of HelloWorldAgent class.\n",
    "num_products = 10\n",
    "agent = HelloWorldAgent(Configuration({\n",
    "    **env_1_args,\n",
    "    'num_products': num_products,\n",
    "}))\n",
    "\n",
    "# Resets random seed back to 42, or whatever we set it to in env_0_args.\n",
    "env.reset_random_seed()\n",
    "\n",
    "# Train on 1000 users offline.\n",
    "num_offline_users = 1000\n",
    "\n",
    "for _ in range(num_offline_users):\n",
    "\n",
    "    # Reset env and set done to False.\n",
    "    env.reset()\n",
    "    done = False\n",
    "\n",
    "    observation, reward, done = None, 0, False\n",
    "    while not done:\n",
    "        old_observation = observation\n",
    "        action, observation, reward, done, info = env.step_offline(observation, reward, done)\n",
    "        agent.train(old_observation, action, reward, done) # TRAIN OFFLINE\n",
    "\n",
    "# Train on 100 users online and track click through rate.\n",
    "num_online_users = 100\n",
    "num_clicks, num_events = 0, 0\n",
    "\n",
    "for _ in range(num_online_users):\n",
    "\n",
    "    # Reset env and set done to False.\n",
    "    env.reset()\n",
    "    observation, _, done, _ = env.step(None)\n",
    "    reward = None\n",
    "    done = None\n",
    "\n",
    "    while not done: # ----- LOOP\n",
    "\n",
    "        action = agent.act(observation, reward, done) # create recommendation\n",
    "\n",
    "        observation, reward, done, info = env.step(action['a'])\n",
    "\n",
    "        # Used for calculating click through rate.\n",
    "        num_clicks += 1 if reward == 1 and reward is not None else 0\n",
    "        num_events += 1\n",
    "\n",
    "ctr = num_clicks / num_events\n",
    "\n",
    "print(f\"Click Through Rate: {ctr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our hello world agent\n",
    "\n",
    "Now we have created our hello world agent, and we should test it against an even simpler baseline -\n",
    "one that performs no learning and recommends products uniformly at random.\n",
    "To do this, we will first load a more complex version of the toy data environment called `reco-gym-v1`.\n",
    "\n",
    "Next, we will load another agent for our agent to compete against each other.\n",
    "Here you can see we make use of the `RandomAgent` and create an instance of it in addition to our `HelloWorldAgent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, recogym\n",
    "from recogym import env_1_args\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "env_1_args['random_seed'] = 42\n",
    "\n",
    "env = gym.make('reco-gym-v1')\n",
    "env.init_gym(env_1_args)\n",
    "\n",
    "# Import the random agent.\n",
    "from recogym.agents import RandomAgent, random_args\n",
    "\n",
    "# Create the two agents.\n",
    "num_products = env_1_args['num_products']\n",
    "\n",
    "agent_hello_world = HelloWorldAgent(Configuration(env_1_args))\n",
    "\n",
    "agent_random = RandomAgent(Configuration({\n",
    "    **env_1_args,\n",
    "    **random_args,\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have instances of our two agents. We can use the `test_agent` method from RecoGym and compare there performance.\n",
    "\n",
    "To use `test_agent`, one must provide a copy of the current env, a copy of the agent class, the number of training users and the number of testing users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Organic Users: 0it [00:00, ?it/s]\n",
      "Users: 100%|██████████| 1000/1000 [00:14<00:00, 68.94it/s]\n",
      "Organic Users: 0it [00:00, ?it/s]\n",
      "Users: 100%|██████████| 1000/1000 [00:15<00:00, 64.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START: Agent Training #0\n",
      "START: Agent Training @ Epoch #0\n",
      "END: Agent Training @ Epoch #0 (14.512444972991943s)\n",
      "START: Agent Evaluating @ Epoch #0\n",
      "END: Agent Evaluating @ Epoch #0 (15.659083843231201s)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.011463611074356707, 0.010743287419571399, 0.012215054884766707)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Credible interval of the CTR median and 0.025 0.975 quantile.\n",
    "\n",
    "recogym.test_agent(deepcopy(env), deepcopy(agent_random), 1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Organic Users: 0it [00:00, ?it/s]\n",
      "Users: 100%|██████████| 1000/1000 [00:14<00:00, 68.88it/s]\n",
      "Organic Users: 0it [00:00, ?it/s]\n",
      "Users: 100%|██████████| 1000/1000 [00:17<00:00, 56.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START: Agent Training #0\n",
      "START: Agent Training @ Epoch #0\n",
      "END: Agent Training @ Epoch #0 (14.523025751113892s)\n",
      "START: Agent Evaluating @ Epoch #0\n",
      "END: Agent Evaluating @ Epoch #0 (17.89901900291443s)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.01419748696611417, 0.013389659432901099, 0.015036695334252781)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Credible interval of the CTR median and 0.025 0.975 quantile.\n",
    "\n",
    "recogym.test_agent(deepcopy(env), deepcopy(agent_hello_world), 1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see an improvement in the click-through rate for the hello world agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}