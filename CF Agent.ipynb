{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import recogym\n",
    "from recogym import Configuration\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "from recogym.agents import Agent\n",
    "\n",
    "import pandas as pd\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "\n",
    "ratings_dict = {\n",
    "\n",
    "    \"product\": [],\n",
    "    \"user\": [],\n",
    "    \"rating\": [],\n",
    "}\n",
    "\n",
    "class SingleActionAgent(Agent):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super(SingleActionAgent, self).__init__(config) # Set number of products as an attribute of the Agent.\n",
    "\n",
    "        self.organic_views = np.zeros(self.config.num_products) # Track number of times each item viewed in Organic session.\n",
    "\n",
    "        self.act_counter = 0\n",
    "        self.train_counter = 0\n",
    "\n",
    "    def train(self, observation, action, reward, done):\n",
    "\n",
    "        # Train method learns from a tuple of data. This method can be called for offline or online learning\n",
    "        # Adding organic session to organic view counts.\n",
    "\n",
    "        if observation:\n",
    "\n",
    "            for session in observation.sessions(): # -- LOOP\n",
    "\n",
    "                print(f\"train () :::: session {session}\")\n",
    "                \n",
    "                ratings_dict['product'].append(session['v']) # viewed product\n",
    "                ratings_dict['user'].append(session['u'])\n",
    "                ratings_dict['rating'].append(1) # scale 1 - organic view, 0 - no view \n",
    "\n",
    "                print(\"\\n-------------- TRAIN START --------------\")\n",
    "                \n",
    "                print(f\"train () {self.train_counter} :::: reward {reward}\")\n",
    "\n",
    "                self.organic_views[session['v']] += 1\n",
    "\n",
    "                print(f\"train () {self.train_counter} :::: self.organic_views {self.organic_views}\")\n",
    "\n",
    "                self.train_counter +=1\n",
    "\n",
    "                print(\"-------------- TRAIN END --------------\\n\")\n",
    "\n",
    "        print(f\"train () TRAIN FINISHED ratings_dict {ratings_dict}\")\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "\n",
    "        #-------------------------------------------------------------------\n",
    "        # INSTEAD OF TRAIN THE AGENT - USE COLLABORATIVE FILTERING FRAMEWORK\n",
    "        #-------------------------------------------------------------------\n",
    "\n",
    "        print(\"\\n-------------- ACT START --------------\")\n",
    "\n",
    "        # An act method takes in an observation, which could either be `None` or an Organic_Session\n",
    "        # and returns a integer between 0 and num_products indicating which product the agent recommends.\n",
    "\n",
    "        print(f\"act () {self.act_counter} :::: get reward {reward}\")\n",
    "        print(f\"act () {self.act_counter} :::: get observation sessions {observation.sessions()}\")\n",
    "        print(f\"act () {self.act_counter} :::: have organic_views {self.organic_views}\")\n",
    "        print(f\"act () {self.act_counter} :::: have sum(self.organic_views) {sum(self.organic_views)}\")\n",
    "\n",
    "        prob = self.organic_views / sum(self.organic_views)\n",
    "\n",
    "        print(f\"act () {self.act_counter} :::: calc prob {prob}\")\n",
    "\n",
    "        print(f\"act () {self.act_counter} :::: have num_products {num_products}\")\n",
    "\n",
    "        #--------------------------------------------------\n",
    " \n",
    "        # action = choice(self.config.num_products, p = prob) # Choosing action RANDOMLY in proportion with number of views.\n",
    "\n",
    "        action = 1\n",
    "\n",
    "        # TODO: create action based on CF Surprise lib recommendation:\n",
    "        # userId, Propensity Scores => MOST RELEVANT Product Id\n",
    "        \n",
    "        #--------------------------------------------------\n",
    "\n",
    "        print(f\"act () {self.act_counter} :::: return action {action}\")\n",
    "        print(f\"act () {self.act_counter} :::: return prob[action] {prob[action]}\")\n",
    "\n",
    "        self.act_counter += 1\n",
    "\n",
    "        print(\"-------------- ACT END --------------\\n\")\n",
    "\n",
    "        return {\n",
    "            **super().act(observation, reward, done),\n",
    "            **{\n",
    "                'a': action,\n",
    "                'ps': prob[action]\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train () TRAIN FINISHED ratings_dict {'product': [], 'user': [], 'rating': []}\n",
      "train () :::: session {'t': 0, 'u': 1, 'z': 'pageview', 'v': 85}\n",
      "\n",
      "-------------- TRAIN START --------------\n",
      "train () 0 :::: reward 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 85 is out of bounds for axis 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c7a9873171d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_offline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mcf_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TRAIN OFFLINE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# ****** TRAIN ONLINE ******\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a24f4d3102bb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, observation, action, reward, done)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"train () {self.train_counter} :::: reward {reward}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morganic_views\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"train () {self.train_counter} :::: self.organic_views {self.organic_views}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 85 is out of bounds for axis 0 with size 10"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from recogym import env_1_args\n",
    "\n",
    "env_1_args['random_seed'] = 42\n",
    "\n",
    "env_1_args['num_steps'] = 10\n",
    "\n",
    "env = gym.make('reco-gym-v1')\n",
    "\n",
    "env.init_gym(env_1_args)\n",
    "\n",
    "# Instantiate instance of SingleActionAgent class.\n",
    "\n",
    "num_products = 10\n",
    "\n",
    "num_offline_users = 5\n",
    "num_online_users = 5\n",
    "\n",
    "cf_agent = SingleActionAgent(Configuration({\n",
    "    **env_1_args,\n",
    "    'num_products': num_products,\n",
    "}))\n",
    "\n",
    "# Resets random seed back to 42, or whatever we set it to in env_0_args.\n",
    "\n",
    "env.reset_random_seed()\n",
    "\n",
    "num_clicks, num_events = 0, 0\n",
    "\n",
    "# ****** TRAIN OFFLINE ******\n",
    "\n",
    "unique_offline_user_id = 0\n",
    "\n",
    "for _ in range(num_offline_users):\n",
    "\n",
    "    # Reset env and set done to False.\n",
    "    \n",
    "    unique_offline_user_id += 1\n",
    "    \n",
    "    env.reset(unique_offline_user_id)\n",
    "    \n",
    "    done = False\n",
    "    observation, reward, done = None, 0, False\n",
    "\n",
    "    while not done: # ----- LOOP\n",
    "\n",
    "        old_observation = observation\n",
    "\n",
    "        action, observation, reward, done, info = env.step_offline(observation, reward, done)\n",
    "\n",
    "        cf_agent.train(old_observation, action, reward, done) # TRAIN OFFLINE\n",
    "\n",
    "# ****** TRAIN ONLINE ******\n",
    "\n",
    "unique_online_user_id = 0\n",
    "\n",
    "for _ in range(num_online_users):\n",
    "\n",
    "    # Reset env and set done to False.\n",
    "    \n",
    "    unique_online_user_id += 1\n",
    "    \n",
    "    env.reset(unique_online_user_id)\n",
    "    \n",
    "    observation, _, done, _ = env.step(None)\n",
    "    reward = None\n",
    "\n",
    "    while not done: # ----- LOOP\n",
    "\n",
    "        print(\"\\n========================== ONLINE LOOP START ===========================\")\n",
    "\n",
    "        print(f\"ONLINE %%%% ratings_dict {ratings_dict}\")\n",
    "\n",
    "        print(f\"ONLINE %%%% RUN act() obs sessions {observation.sessions()}\")\n",
    "\n",
    "        action = cf_agent.act(observation, reward, done) # agent creates recommendation product id\n",
    "\n",
    "        print(f\"ONLINE %%%% action {action}, obs sessions {observation.sessions()} --> RUN step()\")\n",
    "\n",
    "        observation, reward, done, info = env.step(action['a'])\n",
    "\n",
    "        print(f\"ONLINE %%%% calc obs sessions {observation.sessions()}, reward {reward}\")\n",
    "\n",
    "        # Used for calculating click through rate.\n",
    "\n",
    "        num_clicks += 1 if reward == 1 and reward is not None else 0\n",
    "        num_events += 1\n",
    "\n",
    "        print(f\"ONLINE %%%% num_clicks: {num_clicks}\")\n",
    "        print(f\"ONLINE %%%% num_events: {num_events}\")\n",
    "        print(f\"ONLINE %%%% CTR {num_clicks / num_events}\")\n",
    "\n",
    "        print(\"============================== ONLINE LOOP END ============================\\n\")\n",
    "\n",
    "ctr = num_clicks / num_events\n",
    "\n",
    "# ------------------------------------\n",
    "\n",
    "print(f\"TOTAL num_clicks: {num_clicks}\")\n",
    "print(f\"TOTAL num_events: {num_events}\")\n",
    "print(f\"TOTAL Click Through Rate: {ctr:.4f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
