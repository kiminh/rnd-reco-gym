{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random.mtrand import RandomState\n",
    "from scipy.special import logsumexp\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from scipy.stats.distributions import beta\n",
    "from copy import deepcopy\n",
    "import gym, recogym\n",
    "from recogym import env_1_args, Configuration\n",
    "from recogym.agents import OrganicUserEventCounterAgent, organic_user_count_args\n",
    "from recogym.agents.organic_count import OrganicCount, organic_count_args, to_categorical\n",
    "from recogym import Configuration\n",
    "from recogym.agents import Agent\n",
    "from recogym.envs.observation import Observation\n",
    "from recogym.agents import RandomAgent, random_args\n",
    "from recogym import verify_agents, verify_agents_IPS\n",
    "from recogym.evaluate_agent import plot_verify_agents, verify_agents_recall_at_k\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set style for pretty plots\n",
    "\n",
    "P = 2000 # Number of Products\n",
    "U = 2000 # Number of Users\n",
    "\n",
    "# You can overwrite environment arguments here:\n",
    "env_1_args['random_seed'] = 42\n",
    "env_1_args['num_products']= P\n",
    "env_1_args['phi_var']=0.0\n",
    "env_1_args['number_of_flips']=P//2\n",
    "env_1_args['sigma_mu_organic'] = 0.1\n",
    "env_1_args['sigma_omega']=0.05\n",
    "# Initialize the gym for the first time by calling .make() and .init_gym()\n",
    "env = gym.make('reco-gym-v1')\n",
    "env.init_gym(env_1_args)\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate RecSys logs for U users\n",
    "reco_log = env.generate_logs(U)\n",
    "reco_log.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_events = reco_log.shape[0]\n",
    "n_organic = reco_log.loc[reco_log['z'] == 'organic'].shape[0]\n",
    "print('Training on {0} organic and {1} bandit events'.format(n_organic, n_events - n_organic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recogym.envs.session import OrganicSessions\n",
    "from recogym.envs.context import DefaultContext\n",
    "from recogym.envs.observation import Observation\n",
    "\n",
    "def leave_one_out(reco_log, agent, last = False, N = 1, folds = 10):\n",
    "    # 1. Extract all organic events\n",
    "    reco_log = reco_log.loc[reco_log['z'] == 'organic']\n",
    "    \n",
    "    # 2. For every user sequence - randomly sample out an item\n",
    "    hits = []\n",
    "    for _ in range(folds):\n",
    "        user_id = 0\n",
    "        history = []\n",
    "        session = OrganicSessions()\n",
    "        agent.reset()\n",
    "        for row in reco_log.itertuples():\n",
    "            # If we have a new user\n",
    "            if row.u != user_id:\n",
    "                if last:\n",
    "                    # Sample out last item\n",
    "                    index = len(history) - 1\n",
    "                else:\n",
    "                    # Sample out a random item from the history\n",
    "                    index = np.random.choice(len(history),\n",
    "                                             replace = False)\n",
    "                test  = history[index]\n",
    "                train = history[:index] + history[index + 1:]\n",
    "\n",
    "                # 3. Recreate the user sequence without these items - Let the agent observe the incomplete sequence\n",
    "                for t, v in list(train):\n",
    "                    session.next(DefaultContext(t, user_id), int(v))\n",
    "\n",
    "                # 4. Generate a top-N set of recommendations by letting the agent act\n",
    "                # TODO - For now only works for N = 1\n",
    "                try:\n",
    "                    prob_a = agent.act(Observation(DefaultContext(t + 1, user_id), session), 0, False)['ps-a']\n",
    "                except:\n",
    "                    prob_a = [1 / P] * P\n",
    "\n",
    "                # 5. Compute metrics checking whether the sampled test item is in the top-N\n",
    "                hits.append(np.argmax(prob_a) == int(test[1]))\n",
    "\n",
    "                # Reset variables\n",
    "                user_id = row.u\n",
    "                history = []\n",
    "                session = OrganicSessions()\n",
    "                agent.reset()\n",
    "\n",
    "            # Save the organic interaction to the running average for the session\n",
    "            history.append((row.t,row.v))\n",
    "    \n",
    "    # Error analysis\n",
    "    mean_hits = np.mean(hits)\n",
    "    serr_hits = np.std(hits) / np.sqrt(len(hits))\n",
    "    low_bound = mean_hits - 1.96 * serr_hits\n",
    "    upp_bound = mean_hits + 1.96 * serr_hits\n",
    "    \n",
    "    return mean_hits, low_bound, upp_bound\n",
    "\n",
    "def verify_agents_traditional(reco_log, agents, last = False, N = 1, folds = 10):\n",
    "    # Placeholder DataFrame for result\n",
    "    stat = {\n",
    "        'Agent': [],\n",
    "        '0.025': [],\n",
    "        '0.500' : [],\n",
    "        '0.975': [],\n",
    "    }\n",
    "\n",
    "    # For every agent\n",
    "    for agent_id in agents:\n",
    "        # Compute HR@k\n",
    "        mean, low, upp = leave_one_out(reco_log, agents[agent_id], last = last, N = N, folds = folds)\n",
    "        stat['Agent'].append(agent_id)\n",
    "        stat['0.025'].append(low)\n",
    "        stat['0.500'].append(mean)\n",
    "        stat['0.975'].append(upp)\n",
    "    return pd.DataFrame().from_dict(stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ips_weights(agent, reco_log):\n",
    "    # Placeholder for return values\n",
    "    rewards = [] # Labels for actions\n",
    "    t_props = [] # Treatment propensities\n",
    "    l_props = [] # Logging propensities\n",
    "    \n",
    "    # For every logged interaction\n",
    "    user_id = 0\n",
    "    session = OrganicSessions()\n",
    "    agent.reset()\n",
    "    for row in reco_log.itertuples():\n",
    "        # If we have a new user\n",
    "        if row.u != user_id:\n",
    "            # Reset\n",
    "            session = OrganicSessions()\n",
    "            agent.reset()\n",
    "            user_id = row.u\n",
    "        \n",
    "        # If we have an organic event\n",
    "        if row.z == 'organic':\n",
    "            session.next(DefaultContext(row.t, row.u), int(row.v))        \n",
    "            \n",
    "        else:\n",
    "            prob_a = agent.act(Observation(DefaultContext(row.t, row.u), session), 0, False)['ps-a']\n",
    "            rewards.append(row.c)\n",
    "            t_props.append(prob_a[int(row.a)])\n",
    "            l_props.append(row.ps)\n",
    "            session = OrganicSessions()\n",
    "    \n",
    "    return np.asarray(rewards), np.asarray(t_props), np.asarray(l_props)\n",
    "\n",
    "def verify_agents_counterfactual(reco_log, agents, cap = 3):\n",
    "    # Placeholder DataFrame for results\n",
    "    IPS_stat = {\n",
    "        'Agent': [],\n",
    "        '0.025': [],\n",
    "        '0.500' : [],\n",
    "        '0.975': [],\n",
    "    }\n",
    "    CIPS_stat = {\n",
    "        'Agent': [],\n",
    "        '0.025': [],\n",
    "        '0.500' : [],\n",
    "        '0.975': [],\n",
    "    }\n",
    "    SNIPS_stat = {\n",
    "        'Agent': [],\n",
    "        '0.025': [],\n",
    "        '0.500' : [],\n",
    "        '0.975': [],\n",
    "    }\n",
    "\n",
    "    # For every agent\n",
    "    for agent_id in agents:\n",
    "        # Get the rewards and propensities\n",
    "        rewards, t_props, l_props = compute_ips_weights(agents[agent_id], reco_log)\n",
    "        \n",
    "        # Compute the sample weights - propensity ratios\n",
    "        p_ratio = t_props / l_props\n",
    "\n",
    "        # Effective sample size for E_t estimate (from A. Owen)\n",
    "        n_e = len(rewards) * (np.mean(p_ratio) ** 2) / (p_ratio ** 2).mean()\n",
    "        \n",
    "        # Critical value from t-distribution as we have unknown variance\n",
    "        alpha = .00125\n",
    "        cv = scipy.stats.t.ppf(1 - alpha, df = int(n_e) - 1)\n",
    "        \n",
    "        ###############\n",
    "        # VANILLA IPS #\n",
    "        ###############\n",
    "        # Expected reward for pi_t\n",
    "        E_t = np.mean(rewards * p_ratio)\n",
    "\n",
    "        # Variance of the estimate\n",
    "        var = ((rewards * p_ratio - E_t) ** 2).mean()\n",
    "        stddev = np.sqrt(var)\n",
    "        \n",
    "        # C.I. assuming unknown variance - use t-distribution and effective sample size\n",
    "        min_bound = E_t - cv * stddev / np.sqrt(int(n_e))\n",
    "        max_bound = E_t + cv * stddev / np.sqrt(int(n_e))\n",
    "        \n",
    "        # Store result\n",
    "        IPS_stat['Agent'].append(agent_id)\n",
    "        IPS_stat['0.025'].append(min_bound)\n",
    "        IPS_stat['0.500'].append(E_t)\n",
    "        IPS_stat['0.975'].append(max_bound)\n",
    "        \n",
    "        ############## \n",
    "        # CAPPED IPS #\n",
    "        ##############\n",
    "        # Cap ratios\n",
    "        p_ratio_capped = np.clip(p_ratio, a_min = None, a_max = cap)\n",
    "        \n",
    "        # Expected reward for pi_t\n",
    "        E_t_capped = np.mean(rewards * p_ratio_capped)\n",
    "\n",
    "        # Variance of the estimate\n",
    "        var_capped = ((rewards * p_ratio_capped - E_t_capped) ** 2).mean()\n",
    "        stddev_capped = np.sqrt(var_capped)        \n",
    "        \n",
    "        # C.I. assuming unknown variance - use t-distribution and effective sample size\n",
    "        min_bound_capped = E_t_capped - cv * stddev_capped / np.sqrt(int(n_e))\n",
    "        max_bound_capped = E_t_capped + cv * stddev_capped / np.sqrt(int(n_e))\n",
    "        \n",
    "        # Store result\n",
    "        CIPS_stat['Agent'].append(agent_id)\n",
    "        CIPS_stat['0.025'].append(min_bound_capped)\n",
    "        CIPS_stat['0.500'].append(E_t_capped)\n",
    "        CIPS_stat['0.975'].append(max_bound_capped)\n",
    "        \n",
    "        ##############\n",
    "        # NORMED IPS #\n",
    "        ##############\n",
    "        # Expected reward for pi_t\n",
    "        E_t_normed = np.sum(rewards * p_ratio) / np.sum(p_ratio)\n",
    "\n",
    "        # Variance of the estimate\n",
    "        var_normed = np.sum(((rewards - E_t_normed) ** 2) * (p_ratio ** 2)) / (p_ratio.sum() ** 2)    \n",
    "        stddev_normed = np.sqrt(var_normed)\n",
    "\n",
    "        # C.I. assuming unknown variance - use t-distribution and effective sample size\n",
    "        min_bound_normed = E_t_normed - cv * stddev_normed / np.sqrt(int(n_e))\n",
    "        max_bound_normed = E_t_normed + cv * stddev_normed / np.sqrt(int(n_e))\n",
    "\n",
    "        # Store result\n",
    "        SNIPS_stat['Agent'].append(agent_id)\n",
    "        SNIPS_stat['0.025'].append(min_bound_normed)\n",
    "        SNIPS_stat['0.500'].append(E_t_normed)\n",
    "        SNIPS_stat['0.975'].append(max_bound_normed)\n",
    "        \n",
    "    return pd.DataFrame().from_dict(IPS_stat), pd.DataFrame().from_dict(CIPS_stat), pd.DataFrame().from_dict(SNIPS_stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple SVD Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "class SVDAgent(Agent):\n",
    "    def __init__(self, config, U = U, P = P, K = 5):\n",
    "        super(SVDAgent, self).__init__(config)\n",
    "        self.rng = RandomState(self.config.random_seed)\n",
    "        assert(P >= K)\n",
    "        self.K = K\n",
    "        self.R = csr_matrix((U,P))\n",
    "        self.V = np.zeros((P,K))\n",
    "        self.user_history = np.zeros(P)\n",
    "    \n",
    "    def train(self, reco_log, U = U, P = P):\n",
    "        # Extract all organic user logs\n",
    "        reco_log = reco_log.loc[reco_log['z'] == 'organic']\n",
    "        \n",
    "        # Generate ratings matrix for training, row-based for efficient row (user) retrieval\n",
    "        self.R = csr_matrix((np.ones(len(reco_log)),\n",
    "                            (reco_log['u'],reco_log['v'])),\n",
    "                            (U,P))\n",
    "\n",
    "        # Singular Value Decomposition\n",
    "        _, _, self.V = svds(self.R, k = self.K)\n",
    "        \n",
    "    def observe(self, observation):\n",
    "        for session in observation.sessions():\n",
    "            self.user_history[session['v']] += 1\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\"Act method returns an Action based on current observation and past history\"\"\"\n",
    "        self.observe(observation)\n",
    "        scores = self.user_history.dot(self.V.T).dot(self.V)\n",
    "        action = np.argmax(scores)\n",
    "        prob = np.zeros_like(scores)\n",
    "        prob[action] = 1.0\n",
    "\n",
    "        return {\n",
    "            **super().act(observation, reward, done),\n",
    "            **{\n",
    "                'a': action,\n",
    "                'ps': prob[action],\n",
    "                'ps-a': prob,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        self.user_history = np.zeros(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple item-kNN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "class itemkNNAgent(Agent):\n",
    "    def __init__(self, config, U = U, P = P, k = 5, greedy = False, alpha = 1):\n",
    "        super(itemkNNAgent, self).__init__(config)\n",
    "        self.rng = RandomState(self.config.random_seed)\n",
    "        self.k = min(P,k)\n",
    "        self.greedy = greedy\n",
    "        self.alpha = alpha\n",
    "        self.Rt = csr_matrix((P,U))\n",
    "        self.user_history = np.zeros(P)\n",
    "        self.S = np.eye(P)\n",
    "    \n",
    "    def train(self, reco_log, U = U, P = P):\n",
    "        # Extract all organic user logs\n",
    "        reco_log = reco_log.loc[reco_log['z'] == 'organic']\n",
    "        \n",
    "        # Generate ratings matrix for training, row-based for efficient row (user) retrieval\n",
    "        self.R_t = csr_matrix((np.ones(len(reco_log)),\n",
    "                              (reco_log['v'],reco_log['u'])),\n",
    "                              (P,U))\n",
    "\n",
    "        # Set up nearest neighbours module\n",
    "        nn = NearestNeighbors(n_neighbors = self.k,\n",
    "                              metric = 'cosine')\n",
    "\n",
    "        # Initialise placeholder for distances and indices\n",
    "        distances = []\n",
    "        indices   = []\n",
    "\n",
    "        # Dirty fix for multiprocessing backend being unable to pickle large objects\n",
    "        nn.fit(self.R_t)\n",
    "        distances, indices = nn.kneighbors(self.R_t, return_distance = True)\n",
    "\n",
    "        # Precompute similarity matrix S\n",
    "        data = list(chain.from_iterable(1.0 - distances))\n",
    "        rows = list(chain.from_iterable([i] * self.k for i in range(P)))\n",
    "        cols = list(chain.from_iterable(indices))\n",
    "        \n",
    "        # (P,P)-matrix with cosine similarities between items\n",
    "        self.S = csr_matrix((data,(rows, cols)), (P,P)).todense()\n",
    "        \n",
    "    def observe(self, observation):\n",
    "        for session in observation.sessions():\n",
    "            self.user_history[session['v']] += 1\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\"Act method returns an Action based on current observation and past history\"\"\"\n",
    "        self.observe(observation)\n",
    "        scores = self.user_history.dot(self.S).A1\n",
    "        \n",
    "        if self.greedy:\n",
    "            action = np.argmax(scores)\n",
    "            prob = np.zeros_like(scores)\n",
    "            prob[action] = 1.0\n",
    "        else:\n",
    "            scores **= self.alpha\n",
    "            prob = scores / np.sum(scores)\n",
    "            action = self.rng.choice(self.S.shape[0], p = prob)\n",
    "\n",
    "        return {\n",
    "            **super().act(observation, reward, done),\n",
    "            **{\n",
    "                'a': action,\n",
    "                'ps': prob[action],\n",
    "                'ps-a': prob,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        self.user_history = np.zeros(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple user-kNN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class userkNNAgent(Agent):\n",
    "    def __init__(self, config, U = U, P = P, k = 5, greedy = False, alpha = 1):\n",
    "        super(userkNNAgent, self).__init__(config)\n",
    "        self.rng = RandomState(self.config.random_seed)\n",
    "        self.k = min(P,k)\n",
    "        self.greedy = greedy\n",
    "        self.alpha = alpha\n",
    "        self.U = U\n",
    "        self.P = P\n",
    "        self.R = csr_matrix((U,P))\n",
    "        self.user_history = np.zeros(P)\n",
    "        self.nn = NearestNeighbors(n_neighbors = self.k, metric = 'cosine')\n",
    "        \n",
    "    def train(self, reco_log, U = U, P = P):\n",
    "        # Extract all organic user logs\n",
    "        reco_log = reco_log.loc[reco_log['z'] == 'organic']\n",
    "        \n",
    "        # Generate ratings matrix for training, row-based for efficient row (user) retrieval\n",
    "        self.R = csr_matrix((np.ones(len(reco_log)),\n",
    "                            (reco_log['u'],reco_log['v'])),\n",
    "                            (U,P))\n",
    "\n",
    "        # Fit nearest neighbours\n",
    "        self.nn.fit(self.R)\n",
    "        \n",
    "    def observe(self, observation):\n",
    "        for session in observation.sessions():\n",
    "            self.user_history[session['v']] += 1\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\"Act method returns an Action based on current observation and past history\"\"\"\n",
    "        self.observe(observation)\n",
    "        \n",
    "        # Get neighbouring users based on user history\n",
    "        distances, indices = self.nn.kneighbors(self.user_history.reshape(1,-1))\n",
    "        scores = np.add.reduce([dist * self.R[idx,:] for dist, idx in zip(distances,indices)])\n",
    "        \n",
    "        if self.greedy:\n",
    "            action = np.argmax(scores)\n",
    "            prob = np.zeros_like(scores)\n",
    "            prob[action] = 1.0\n",
    "        else:\n",
    "            scores **= self.alpha\n",
    "            prob = scores / np.sum(scores)\n",
    "            action = self.rng.choice(self.P, p = prob)\n",
    "\n",
    "        return {\n",
    "            **super().act(observation, reward, done),\n",
    "            **{\n",
    "                'a': action,\n",
    "                'ps': prob[action],\n",
    "                'ps-a': prob,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        self.user_history = np.zeros(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise and train agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# SVD Agent\n",
    "SVD_agent = SVDAgent(Configuration(env_1_args), U, P, 30)\n",
    "SVD_agent.train(reco_log)\n",
    "\n",
    "# item-kNN Agent\n",
    "itemkNN_agent = itemkNNAgent(Configuration(env_1_args), U, P, 500, greedy = True)\n",
    "itemkNN_agent.train(reco_log)\n",
    "\n",
    "# user-kNN Agent\n",
    "userkNN_agent = userkNNAgent(Configuration(env_1_args), U, P, 20, greedy = True)\n",
    "userkNN_agent.train(reco_log)\n",
    "\n",
    "# Generalised Popularity agent\n",
    "GPOP_agent = OrganicCount(Configuration({\n",
    "    **env_1_args,\n",
    "    'select_randomly': True,\n",
    "}))\n",
    "\n",
    "# Generalised Popularity agent\n",
    "GPOP_agent_greedy = OrganicCount(Configuration({\n",
    "    **env_1_args,\n",
    "    'select_randomly': False,\n",
    "}))\n",
    "\n",
    "# Peronalised Popularity agent\n",
    "PPOP_agent = OrganicUserEventCounterAgent(Configuration({\n",
    "    **organic_user_count_args,\n",
    "    **env_1_args,\n",
    "    'select_randomly': True,\n",
    "}))\n",
    "\n",
    "# Peronalised Popularity agent\n",
    "PPOP_agent_greedy = OrganicUserEventCounterAgent(Configuration({\n",
    "    **organic_user_count_args,\n",
    "    **env_1_args,\n",
    "    'select_randomly': False,\n",
    "}))\n",
    "\n",
    "# Random Agent\n",
    "random_args['num_products'] = P\n",
    "RAND_agent = RandomAgent(Configuration({**env_1_args, **random_args,}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline Evaluation\n",
    "All of the above is quite recognisable from earlier parts of the course, but now here things differ.\n",
    "We will use the logged feedback we obtained from the recommender system to evaluate the agents we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from IPython.display import display, HTML\n",
    "# Placeholder for agents\n",
    "agents = {\n",
    "    '    Random': RAND_agent,\n",
    "    '   Popular': GPOP_agent_greedy,\n",
    "    '   User-pop': PPOP_agent,\n",
    "    '  SVD':  SVD_agent,\n",
    "    ' User-kNN': userkNN_agent,\n",
    "    'Item-kNN': itemkNN_agent,\n",
    "}\n",
    "agent_ids = sorted(list(agents.keys()))#['SVD','GPOP','PPOP','RAND']\n",
    "# Generate new logs, to be used for offline testing\n",
    "n_test_users = 5000 # U\n",
    "test_log = env.generate_logs(n_test_users)\n",
    "n_events = test_log.shape[0]\n",
    "n_organic = test_log.loc[test_log['z'] == 'organic'].shape[0]\n",
    "print('Testing on {0} organic and {1} bandit events'.format(n_organic, n_events - n_organic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-One-Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "def plot_barchart(result, title, xlabel,  col = 'tab:red', figname = 'fig.eps', size = (6,2), fontsize = 12):\n",
    "    fig, axes = plt.subplots(figsize = size)\n",
    "    plt.title(title, size = fontsize)\n",
    "    n_agents = len(result)\n",
    "    yticks = np.arange(n_agents)\n",
    "    mean = result['0.500']\n",
    "    lower = result['0.500'] - result['0.025']\n",
    "    upper = result['0.975'] - result['0.500']\n",
    "    plt.barh(yticks,\n",
    "             mean,\n",
    "             height = .25,\n",
    "             xerr  = (lower, upper),\n",
    "             align = 'center',\n",
    "             color = col,)\n",
    "    plt.yticks(yticks, result['Agent'], size = fontsize)\n",
    "    plt.xticks(size = fontsize)\n",
    "    plt.xlabel(xlabel, size = fontsize)\n",
    "    plt.xlim(.0,None)\n",
    "    plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "    plt.savefig(figname, bbox_inches = 'tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result_LOO = verify_agents_traditional(test_log, deepcopy(agents))\n",
    "display(result_LOO)\n",
    "plot_barchart(result_LOO, 'Evaluate on Organic Feedback', 'HR@1', 'tab:red', 'traditional_eval.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPS Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\pi_0 = \\text{ppop}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Generate new logs, to be used for offline testing\n",
    "test_log_ppop = env.generate_logs(n_test_users, agent = deepcopy(PPOP_agent))\n",
    "cap = 15\n",
    "result_IPS, result_CIPS, result_SNIPS = verify_agents_counterfactual(test_log_ppop, deepcopy(agents), cap = cap)\n",
    "display(result_IPS)\n",
    "plot_barchart(result_IPS, 'IPS', 'CTR', 'tab:blue', 'bandit_eval_noclip.eps')\n",
    "display(result_CIPS)\n",
    "plot_barchart(result_CIPS, 'Clipped IPS', 'CTR', 'tab:blue', 'bandit_eval_clip{0}.eps'.format(cap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/B-Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result_AB = verify_agents(env, n_test_users, deepcopy(agents))\n",
    "display(result_AB)\n",
    "plot_barchart(result_AB, 'A/B-test', 'CTR', 'tab:green', 'ABtest_eval.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "def combine_barchart(resultAB, resultCIPS, title, xlabel,  figname = 'fig.eps', size = (6,2), fontsize = 12):\n",
    "    fig, axes = plt.subplots(figsize = size)\n",
    "    plt.title(title, size = fontsize)\n",
    "    n_agents = len(resultAB)\n",
    "    \n",
    "    for i, (name, colour, result) in enumerate([('A/B-test', 'tab:green', result_AB),('CIPS', 'tab:blue', result_CIPS)]):\n",
    "        mean = result['0.500']\n",
    "        lower = result['0.500'] - result['0.025']\n",
    "        upper = result['0.975'] - result['0.500']\n",
    "        height = .25\n",
    "        yticks = [a + i * height for a in range(n_agents)]\n",
    "        plt.barh(yticks,\n",
    "                 mean,\n",
    "                 height = height,\n",
    "                 xerr  = (lower, upper),\n",
    "                 align = 'edge',\n",
    "                 label = name,\n",
    "                 color = colour)\n",
    "    plt.yticks(yticks, result['Agent'], size = fontsize)\n",
    "    plt.xticks(size = fontsize)\n",
    "    plt.xlabel(xlabel, size = fontsize)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.xlim(.0,None)\n",
    "    plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "    plt.savefig(figname, bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "combine_barchart(result_AB, result_CIPS, 'Evaluate on Bandit Feedback', 'CTR', 'ABtest_CIPS.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_barchart(result_LOO, 'Evaluate on Organic Feedback', 'HR@1', 'tab:red', 'traditional_eval.eps')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
