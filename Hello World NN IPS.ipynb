{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 0 - ACTION: None - OBSERVATION: [{'t': 0, 'u': 0, 'z': 'pageview', 'v': 85}] - REWARD: None\n",
      "STEP: 1 - ACTION: {'t': 1, 'u': 0, 'a': 99, 'ps': 0.001, 'ps-a': ()} - OBSERVATION: [] - REWARD: 0\n",
      "STEP: 2 - ACTION: {'t': 2, 'u': 0, 'a': 308, 'ps': 0.001, 'ps-a': ()} - OBSERVATION: [] - REWARD: 0\n",
      "STEP: 3 - ACTION: {'t': 3, 'u': 0, 'a': 805, 'ps': 0.001, 'ps-a': ()} - OBSERVATION: [] - REWARD: 0\n"
     ]
    }
   ],
   "source": [
    "import gym, recogym\n",
    "\n",
    "# env_0_args is a dictionary of default parameters (i.e. number of products)\n",
    "from recogym import env_1_args, Configuration\n",
    "\n",
    "# You can overwrite environment arguments here:\n",
    "env_1_args['random_seed'] = 42\n",
    "\n",
    "# Initialize the gym for the first time by calling .make() and .init_gym()\n",
    "env = gym.make('reco-gym-v1')\n",
    "env.init_gym(env_1_args)\n",
    "\n",
    "# .reset() env before each episode (one episode per user).\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "# Counting how many steps.\n",
    "i = 0\n",
    "\n",
    "observation, reward, done = None, 0, False\n",
    "while not done:\n",
    "    action, observation, reward, done, info = env.step_offline(observation, reward, done)\n",
    "    print(f\"STEP: {i} - ACTION: {action} - OBSERVATION: {observation.sessions()} - REWARD: {reward}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 - Action: None - Observation: [{'t': 0, 'u': 0, 'z': 'pageview', 'v': 796}] - Reward: None\n",
      "Step: 1 - Action: 1 - Observation: [] - Reward: 0\n",
      "Step: 2 - Action: 2 - Observation: [] - Reward: 0\n",
      "Step: 3 - Action: 3 - Observation: [] - Reward: 0\n",
      "Step: 4 - Action: 4 - Observation: [] - Reward: 0\n",
      "Step: 5 - Action: 5 - Observation: [{'t': 6, 'u': 0, 'z': 'pageview', 'v': 796}, {'t': 7, 'u': 0, 'z': 'pageview', 'v': 796}, {'t': 8, 'u': 0, 'z': 'pageview', 'v': 796}, {'t': 9, 'u': 0, 'z': 'pageview', 'v': 796}, {'t': 10, 'u': 0, 'z': 'pageview', 'v': 796}, {'t': 11, 'u': 0, 'z': 'pageview', 'v': 796}, {'t': 12, 'u': 0, 'z': 'pageview', 'v': 796}, {'t': 13, 'u': 0, 'z': 'pageview', 'v': 796}, {'t': 14, 'u': 0, 'z': 'pageview', 'v': 796}, {'t': 15, 'u': 0, 'z': 'pageview', 'v': 796}, {'t': 16, 'u': 0, 'z': 'pageview', 'v': 796}, {'t': 17, 'u': 0, 'z': 'pageview', 'v': 24}, {'t': 18, 'u': 0, 'z': 'pageview', 'v': 796}, {'t': 19, 'u': 0, 'z': 'pageview', 'v': 796}, {'t': 20, 'u': 0, 'z': 'pageview', 'v': 796}, {'t': 21, 'u': 0, 'z': 'pageview', 'v': 796}, {'t': 22, 'u': 0, 'z': 'pageview', 'v': 546}, {'t': 23, 'u': 0, 'z': 'pageview', 'v': 796}] - Reward: 0\n",
      "Step: 6 - Action: 6 - Observation: [] - Reward: 0\n",
      "Step: 7 - Action: 7 - Observation: [] - Reward: 0\n",
      "Step: 8 - Action: 8 - Observation: [] - Reward: 0\n"
     ]
    }
   ],
   "source": [
    "# Create list of hard coded actions.\n",
    "actions = [None] + [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# Reset env and set done to False.\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "# Counting how many steps.\n",
    "i = 0\n",
    "\n",
    "while not done and i < len(actions):\n",
    "    action = actions[i]\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(f\"Step: {i} - Action: {action} - Observation: {observation.sessions()} - Reward: {reward}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recogym.agents import NnIpsAgent, nn_ips_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-5c6dcfb28c0b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     23\u001B[0m         \u001B[0mold_observation\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mobservation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0maction\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobservation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep_offline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m         \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mold_observation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# TRAIN OFFLINE\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;31m# Train on 100 users online and track click through rate.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/workspace/ML/reco-gym/recogym/agents/abstract.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, observation, action, reward, done)\u001B[0m\n\u001B[1;32m    301\u001B[0m         \u001B[0;31m#      print(f\"agents/abstract: TRAIN() sessions #{observation.sessions()}\")\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 303\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel_builder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    304\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    305\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mact\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobservation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/workspace/ML/reco-gym/recogym/agents/abstract.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, observation, action, reward, done)\u001B[0m\n\u001B[1;32m     67\u001B[0m         \u001B[0;34m:\u001B[0m\u001B[0;32mreturn\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mnothing\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     68\u001B[0m         \"\"\"\n\u001B[0;32m---> 69\u001B[0;31m         \u001B[0;32massert\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     70\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msessions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0msession\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mobservation\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msessions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Instantiate instance of HelloWorldAgent class.\n",
    "num_products = 10\n",
    "\n",
    "agent = NnIpsAgent(Configuration({\n",
    "    **nn_ips_args,\n",
    "    **env_1_args,\n",
    "}))\n",
    "\n",
    "# Resets random seed back to 42, or whatever we set it to in env_0_args.\n",
    "env.reset_random_seed()\n",
    "\n",
    "# Train on 1000 users offline.\n",
    "num_offline_users = 1000\n",
    "\n",
    "for _ in range(num_offline_users):\n",
    "\n",
    "    # Reset env and set done to False.\n",
    "    env.reset()\n",
    "    done = False\n",
    "\n",
    "    observation, reward, done = None, 0, False\n",
    "    while not done:\n",
    "        old_observation = observation\n",
    "        action, observation, reward, done, info = env.step_offline(observation, reward, done)\n",
    "        agent.train(old_observation, action, reward, done) # TRAIN OFFLINE\n",
    "\n",
    "# Train on 100 users online and track click through rate.\n",
    "num_online_users = 100\n",
    "num_clicks, num_events = 0, 0\n",
    "\n",
    "for _ in range(num_online_users):\n",
    "\n",
    "    # Reset env and set done to False.\n",
    "    env.reset()\n",
    "    observation, _, done, _ = env.step(None)\n",
    "    reward = None\n",
    "    done = None\n",
    "\n",
    "    while not done: # ----- LOOP\n",
    "\n",
    "        action = agent.act(observation, reward, done) # create recommendation\n",
    "\n",
    "        observation, reward, done, info = env.step(action['a'])\n",
    "\n",
    "        # Used for calculating click through rate.\n",
    "        num_clicks += 1 if reward == 1 and reward is not None else 0\n",
    "        num_events += 1\n",
    "\n",
    "ctr = num_clicks / num_events\n",
    "\n",
    "print(f\"Click Through Rate: {ctr:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}